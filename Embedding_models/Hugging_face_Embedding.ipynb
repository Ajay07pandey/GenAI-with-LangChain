## Vector Embeddings of a Sentence.

from langchain_huggingface import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')

text = "Delhi is the capital of india"

text_vector = embedding.embed_query(text)

print(str(text_vector))
### Checking the vector embedding
import numpy as np

text_array = np.array(text_vector)

print(text_array.shape)

## Vector embedding of a documents (List of sentence.)
from langchain_huggingface import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')

document = [
    "Delhi is the capital of India",
    "kolkata is the capital of West Bengal",
    "Paris is the capital of france"
]

doc_vector = embedding.embed_documents(document)

print(doc_vector)
### checking the shape of the vector
import numpy as np

# Convert to NumPy array to check the shape
doc_vector_np = np.array(doc_vector)

print("Shape of embedding matrix:", doc_vector_np.shape)

for 3 input documents and the model all-MiniLM-L6-v2 will produce 384-dimensional embeddings
## Embedding Model code for Document similarity
Here we will generate the embeddings of 5 documents and then if a user will ask the question then we will try to know from which embeddings the question is closely related to.

Now we will check the similarity between the 5 vector embeddings from our question embedding vector i.e for which embedding of the question this vector is closely related to suppose the vectors are of 385 Dimension.
from langchain_huggingface import HuggingFaceEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')

documents = [
"Virat Kohli is an Indian cricketer known for his aggressive batting and leadership.",
"MS Dhoni is a former Indian captain famous for his calm demeanor and finishing skills.",
"Sachin Tendulkar, also known as the 'God of Cricket', holds many batting records.",
"Rohit Sharma is known for his elegant batting and record-breaking double centuries.",
"Jasprit Bumrah is an Indian fast bowler known for his unorthodox action and yorkers."
]

query = 'Tell me about virat kohli'

doc_embeddings = embedding.embed_documents(documents)
query_embedding = embedding.embed_query(query)

print(cosine_similarity([query_embedding], doc_embeddings))

# Now the ouput is 2D list but we want only one Dimension list
scores = cosine_similarity([query_embedding], doc_embeddings)[0]

# Now with every score we will attach a number (Index)
print(list(enumerate(scores)))

# Now we will short the list on the basis of similarity score (smallest to largest)
print(sorted(list(enumerate(scores)), key = lambda x:x [1]))

# Now we will get the biggest score or most similar text or vector.(picking largest score)
print(sorted(list(enumerate(scores)), key = lambda x:x [1])[-1])

# Now we will save index and it's score in two different variable
index, score = (sorted(list(enumerate(scores)), key = lambda x:x [1])[-1])

print(query)
print(documents[index])
print("similarity score is : ", score)

from langchain_huggingface import HuggingFaceEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')

documents = [
"Virat Kohli is an Indian cricketer known for his aggressive batting and leadership.",
"MS Dhoni is a former Indian captain famous for his calm demeanor and finishing skills.",
"Sachin Tendulkar, also known as the 'God of Cricket', holds many batting records.",
"Rohit Sharma is known for his elegant batting and record-breaking double centuries.",
"Jasprit Bumrah is an Indian fast bowler known for his unorthodox action and yorkers."
]

query = 'Tell me about ajay'

doc_embeddings = embedding.embed_documents(documents)
query_embedding = embedding.embed_query(query)

print(cosine_similarity([query_embedding], doc_embeddings))

# Now the ouput is 2D list but we want only one Dimension list
scores = cosine_similarity([query_embedding], doc_embeddings)[0]

# Now with every score we will attach a number (Index)
print(list(enumerate(scores)))

# Now we will short the list on the basis of similarity score (smallest to largest)
print(sorted(list(enumerate(scores)), key = lambda x:x [1]))

# Now we will get the biggest score or most similar text or vector.(picking largest score)
print(sorted(list(enumerate(scores)), key = lambda x:x [1])[-1])

# Now we will save index and it's score in two different variable
index, score = (sorted(list(enumerate(scores)), key = lambda x:x [1])[-1])

print(query)
print(documents[index])
print("similarity score is : ", score)
